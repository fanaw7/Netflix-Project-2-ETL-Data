# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1E9Neviqu-DHCNtLd8zLwCknGXKl9sV8W
"""

import os

# Find the latest version of spark 3.0  from http://www-us.apache.org/dist/spark/ and enter as the spark version
# For example:
# spark_version = 'spark-3.0.2'
spark_version = 'spark-3.1.1'
os.environ['SPARK_VERSION']=spark_version

# Install Spark and Java
!apt-get update
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q http://www-us.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz
!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz
!pip install -q findspark

# Set Environment Variables
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64"
os.environ["SPARK_HOME"] = f"/content/{spark_version}-bin-hadoop2.7"

# Start a SparkSession
import findspark
findspark.init()

#import packages

from pyspark.sql import SparkSession
from pyspark.sql import Row
from pyspark.sql.types import StructType,StructField,StringType, DateType,IntegerType

# we are going to use this to time our queries.
import time

# Create a SparkSession
spark = SparkSession.builder.appName("SparkSQL").getOrCreate()



# Read in data from S3 Bucket
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-netflix/titles_basic.csv"
spark.sparkContext.addFile(url)
df = spark.read.csv(SparkFiles.get("titles_basic.csv"), sep=",", header=True)

df.createOrReplaceTempView('names')

df2 = spark.sql("SELECT * FROM names")
clean_professions_df=df2.drop("genres","primarytitle","isadult", "startyear", "endyear", "_c0")
clean_professions_df.show()

# Read in data from S3 Bucket
from pyspark import SparkFiles
url = "https://2u-data-curriculum-team.s3.amazonaws.com/dataviz-netflix/ratings.csv"
spark.sparkContext.addFile(url)
df2 = spark.read.csv(SparkFiles.get("ratings.csv"), sep=",", header=True)

df2.createOrReplaceTempView('ratings')

df2 = spark.sql("SELECT * FROM ratings")
clean_ratings_df=df2.drop("numvotes")
clean_ratings_df.show()

complete_df = clean_professions_df.join(clean_ratings_df, on = ['tconst'])
complete_df.show()

from datetime import timedelta
from pyspark.sql.functions import udf
def minutesToHourMinutes(minuteString = "0.0"):
  
  
  if minuteString:
    minutesAsFloat = float(minuteString)
    return str(timedelta(minutes=minutesAsFloat))[:-3]
  else:
    return "0:0"

minutesToHourMinutesUdf = udf(lambda z: minutesToHourMinutes(z), StringType())
spark.udf.register("minutesToHourMinutesUdf", minutesToHourMinutesUdf)

newTitleDf = complete_df.withColumn("formatted_hhmm", minutesToHourMinutesUdf("runtimeminutes"))
newTitleDf.show()



